{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Preprocessing_OnlyDocs.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "VthNTkTIrGCD",
        "9dcSXXYBK-2s",
        "AviW5i5LLH_O",
        "C3ightnwLEN9",
        "w9T9deqcLNRH",
        "Lx5AaCa96wyu",
        "SmeccYkpp_4B"
      ],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sanujsriv/text-data-preprocessing/blob/dummy/Preprocessing_OnlyDocs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQn1j71xqPMG"
      },
      "source": [
        "Preprocessing Steps :- \n",
        "1. Convert to lowercase\n",
        "2. Remove punctuations with empty space \n",
        "3. Remove digits\n",
        "4. Apply lemmatization\n",
        "5. Remove Stopwords\n",
        "6. Remove words that do not have word embeddings\n",
        "7. Remove words that have length < 3.\n",
        "*** NOT APPLYING STEMMING\n",
        "\n",
        "Generating Preprocessed Docs Steps:\n",
        "1. Apply Preprocessing Steps\n",
        "2. Remove Documents that have length < 3\n",
        "3. Apply CountVectorizer Data transform { with min_df =3 i.e. words that appear in less than 3 documents are removed }\n",
        "4. Remove documents that are empty after countvectorization\n",
        "5. Remove words in documents that do not appear in the vocab generated by countvectorization\n",
        "6. Generate embeddings using the vocab \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VthNTkTIrGCD"
      },
      "source": [
        "# Just Run these"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "usMtzpC06t1s",
        "cellView": "form"
      },
      "source": [
        "#@title function : load / save pickle_obj\n",
        "import pickle\n",
        "import csv\n",
        "import pickle5\n",
        "def save_obj(obj, name):\n",
        "    with open(name + '.pkl', 'wb') as f:\n",
        "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "def load_obj(name):\n",
        "    with open(name + '.pkl', 'rb') as f:\n",
        "        return pickle.load(f)\n",
        "\n",
        "def load_obj_pkl5(name):\n",
        "    with open(name + '.pkl', 'rb') as f:\n",
        "        return pickle5.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o-3UyABlrA2K",
        "cellView": "form",
        "outputId": "ddf24b67-b24c-4fa6-88a2-b20005d22ab8"
      },
      "source": [
        "#@title Imports, wordnet,punkt\n",
        "import nltk\n",
        "import os\n",
        "import glob\n",
        "import pandas as pd\n",
        "# nltk.download('stopwords')  \n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "import string\n",
        "from time import time\n",
        "import numpy as np\n",
        "import collections\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from numpy import random\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "from nltk import word_tokenize\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     /home/grad16/sakumar/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     /home/grad16/sakumar/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NPo4cuUR9qI0"
      },
      "source": [
        "import os\n",
        "import pandas\n",
        "if 'grad16' in os.getcwd(): local=True\n",
        "else: local = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17b07E1_-Tj5"
      },
      "source": [
        "#@title If local run this...\n",
        "# if local == True:\n",
        "#   colab_dir= 'C:\\\\Users\\\\sanuj\\\\Documents\\\\FoTo\\\\colab'\n",
        "#   os.chdir(colab_dir)\n",
        "#   directory= 'Preprocessing'\n",
        "#   try : os.mkdir(directory)\n",
        "#   except FileExistsError:pass\n",
        "#   os.chdir(directory)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8PQjaZE5bSIk"
      },
      "source": [
        "# Preprocessing [StopWords,Stemming,Punctuations] & Word2Vec "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zf1p8IblDwIP"
      },
      "source": [
        "stem = PorterStemmer()\n",
        "wnl = WordNetLemmatizer()\n",
        "stopwords = ['la','wa','will','fa','ha','pa','co','v','said','cant','better','well','going','will','would','know','dont','get','like','think','im',\"also\",\"said\",\"a\", \"able\", \"about\", \"above\", \"abst\", \"accordance\", \"according\", \"accordingly\", \"across\", \"act\", \"actually\", \"added\", \"adj\", \"affected\", \"affecting\", \"affects\", \"after\", \"afterwards\", \"again\", \"against\", \"ah\", \"all\", \"almost\", \"alone\", \"along\", \"already\", \"also\", \"although\", \"always\", \"am\", \"among\", \"amongst\", \"an\", \"and\", \"announce\", \"another\", \"any\", \"anybody\", \"anyhow\", \"anymore\", \"anyone\", \"anything\", \"anyway\", \"anyways\", \"anywhere\", \"apparently\", \"approximately\", \"are\", \"aren\", \"arent\", \"arise\", \"around\", \"as\", \"aside\", \"ask\", \"asking\", \"at\", \"auth\", \"available\", \"away\", \"awfully\", \"b\", \"back\", \"be\", \"became\", \"because\", \"become\", \"becomes\", \"becoming\", \"been\", \"before\", \"beforehand\", \"begin\", \"beginning\", \"beginnings\", \"begins\", \"behind\", \"being\", \"believe\", \"below\", \"beside\", \"besides\", \"between\", \"beyond\", \"biol\", \"both\", \"brief\", \"briefly\", \"but\", \"by\", \"c\", \"ca\", \"came\", \"can\", \"cannot\", \"can't\", \"cause\", \"causes\", \"certain\", \"certainly\", \"co\", \"com\", \"come\", \"comes\", \"contain\", \"containing\", \"contains\", \"could\", \"couldnt\", \"d\", \"date\", \"did\", \"didn't\", \"different\", \"do\", \"does\", \"doesn't\", \"doing\", \"done\", \"don't\", \"down\", \"downwards\", \"due\", \"during\", \"e\", \"each\", \"ed\", \"edu\", \"effect\", \"eg\", \"eight\", \"eighty\", \"either\", \"else\", \"elsewhere\", \"end\", \"ending\", \"enough\", \"especially\", \"et\", \"et-al\", \"etc\", \"even\", \"ever\", \"every\", \"everybody\", \"everyone\", \"everything\", \"everywhere\", \"ex\", \"except\", \"f\", \"far\", \"few\", \"ff\", \"fifth\", \"first\", \"five\", \"fix\", \"followed\", \"following\", \"follows\", \"for\", \"former\", \"formerly\", \"forth\", \"found\", \"four\", \"from\", \"further\", \"furthermore\", \"g\", \"gave\", \"get\", \"gets\", \"getting\", \"give\", \"given\", \"gives\", \"giving\", \"go\", \"goes\", \"gone\", \"got\", \"gotten\", \"h\", \"had\", \"happens\", \"hardly\", \"has\", \"hasn't\", \"have\", \"haven't\", \"having\", \"he\", \"hed\", \"hence\", \"her\", \"here\", \"hereafter\", \"hereby\", \"herein\", \"heres\", \"hereupon\", \"hers\", \"herself\", \"hes\", \"hi\", \"hid\", \"him\", \"himself\", \"his\", \"hither\", \"home\", \"how\", \"howbeit\", \"however\", \"hundred\", \"i\", \"id\", \"ie\", \"if\", \"i'll\", \"im\", \"immediate\", \"immediately\", \"importance\", \"important\", \"in\", \"inc\", \"indeed\", \"index\", \"information\", \"instead\", \"into\", \"invention\", \"inward\", \"is\", \"isn't\", \"it\", \"itd\", \"it'll\", \"its\", \"itself\", \"i've\", \"j\", \"just\", \"k\", \"keep  keeps\", \"kept\", \"kg\", \"km\", \"know\", \"known\", \"knows\", \"l\", \"largely\", \"last\", \"lately\", \"later\", \"latter\", \"latterly\", \"least\", \"less\", \"lest\", \"let\", \"lets\", \"like\", \"liked\", \"likely\", \"line\", \"little\", \"'ll\", \"look\", \"looking\", \"looks\", \"ltd\", \"m\", \"made\", \"mainly\", \"make\", \"makes\", \"many\", \"may\", \"maybe\", \"me\", \"mean\", \"means\", \"meantime\", \"meanwhile\", \"merely\", \"mg\", \"might\", \"million\", \"miss\", \"ml\", \"more\", \"moreover\", \"most\", \"mostly\", \"mr\", \"mrs\", \"much\", \"mug\", \"must\", \"my\", \"myself\", \"n\", \"na\", \"name\", \"namely\", \"nay\", \"nd\", \"near\", \"nearly\", \"necessarily\", \"necessary\", \"need\", \"needs\", \"neither\", \"never\", \"nevertheless\", \"new\", \"next\", \"nine\", \"ninety\", \"no\", \"nobody\", \"non\", \"none\", \"nonetheless\", \"noone\", \"nor\", \"normally\", \"nos\", \"not\", \"noted\", \"nothing\", \"now\", \"nowhere\", \"o\", \"obtain\", \"obtained\", \"obviously\", \"of\", \"off\", \"often\", \"oh\", \"ok\", \"okay\", \"old\", \"omitted\", \"on\", \"once\", \"one\", \"ones\", \"only\", \"onto\", \"or\", \"ord\", \"other\", \"others\", \"otherwise\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"outside\", \"over\", \"overall\", \"owing\", \"own\", \"p\", \"page\", \"pages\", \"part\", \"particular\", \"particularly\", \"past\", \"per\", \"perhaps\", \"placed\", \"please\", \"plus\", \"poorly\", \"possible\", \"possibly\", \"potentially\", \"pp\", \"predominantly\", \"present\", \"previously\", \"primarily\", \"probably\", \"promptly\", \"proud\", \"provides\", \"put\", \"q\", \"que\", \"quickly\", \"quite\", \"qv\", \"r\", \"ran\", \"rather\", \"rd\", \"re\", \"readily\", \"really\", \"recent\", \"recently\", \"ref\", \"refs\", \"regarding\", \"regardless\", \"regards\", \"related\", \"relatively\", \"research\", \"respectively\", \"resulted\", \"resulting\", \"results\", \"right\", \"run\", \"s\", \"said\", \"same\", \"saw\", \"say\", \"saying\", \"says\", \"sec\", \"section\", \"see\", \"seeing\", \"seem\", \"seemed\", \"seeming\", \"seems\", \"seen\", \"self\", \"selves\", \"sent\", \"seven\", \"several\", \"shall\", \"she\", \"shed\", \"she'll\", \"shes\", \"should\", \"shouldn't\", \"show\", \"showed\", \"shown\", \"showns\", \"shows\", \"significant\", \"significantly\", \"similar\", \"similarly\", \"since\", \"six\", \"slightly\", \"so\", \"some\", \"somebody\", \"somehow\", \"someone\", \"somethan\", \"something\", \"sometime\", \"sometimes\", \"somewhat\", \"somewhere\", \"soon\", \"sorry\", \"specifically\", \"specified\", \"specify\", \"specifying\", \"still\", \"stop\", \"strongly\", \"sub\", \"substantially\", \"successfully\", \"such\", \"sufficiently\", \"suggest\", \"sup\", \"sure    t\", \"take\", \"taken\", \"taking\", \"tell\", \"tends\", \"th\", \"than\", \"thank\", \"thanks\", \"thanx\", \"that\", \"that'll\", \"thats\", \"that've\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"thence\", \"there\", \"thereafter\", \"thereby\", \"thered\", \"therefore\", \"therein\", \"there'll\", \"thereof\", \"therere\", \"theres\", \"thereto\", \"thereupon\", \"there've\", \"these\", \"they\", \"theyd\", \"they'll\", \"theyre\", \"they've\", \"think\", \"this\", \"those\", \"thou\", \"though\", \"thoughh\", \"thousand\", \"throug\", \"through\", \"throughout\", \"thru\", \"thus\", \"til\", \"tip\", \"to\", \"together\", \"too\", \"took\", \"toward\", \"towards\", \"tried\", \"tries\", \"truly\", \"try\", \"trying\", \"ts\", \"twice\", \"two\", \"u\", \"un\", \"under\", \"unfortunately\", \"unless\", \"unlike\", \"unlikely\", \"until\", \"unto\", \"up\", \"upon\", \"ups\", \"us\", \"use\", \"used\", \"useful\", \"usefully\", \"usefulness\", \"uses\", \"using\", \"usually\", \"v\", \"value\", \"various\", \"'ve\", \"very\", \"via\", \"viz\", \"vol\", \"vols\", \"vs\", \"w\", \"want\", \"wants\", \"was\", \"wasnt\", \"way\", \"we\", \"wed\", \"welcome\", \"we'll\", \"went\", \"were\", \"werent\", \"we've\", \"what\", \"whatever\", \"what'll\", \"whats\", \"when\", \"whence\", \"whenever\", \"where\", \"whereafter\", \"whereas\", \"whereby\", \"wherein\", \"wheres\", \"whereupon\", \"wherever\", \"whether\", \"which\", \"while\", \"whim\", \"whither\", \"who\", \"whod\", \"whoever\", \"whole\", \"who'll\", \"whom\", \"whomever\", \"whos\", \"whose\", \"why\", \"widely\", \"willing\", \"wish\", \"with\", \"within\", \"without\", \"wont\", \"words\", \"world\", \"would\", \"wouldnt\", \"www\", \"x\", \"y\", \"yes\", \"yet\", \"you\", \"youd\", \"you'll\", \"your\", \"youre\", \"yours\", \"yourself\", \"yourselves\", \"you've\", \"z\", \"zero\"]\n",
        " \n",
        "def preprocessing(doc,word2vec_model,my_punctuation,min_word_length=0):\n",
        "    doc = doc.lower()\n",
        "    doc = doc.translate(str.maketrans(string.punctuation, ' '*len(string.punctuation)))  # string.punctuation[6] = ' won't --> won , don't --> don\n",
        "    doc = word_tokenize(doc)    \n",
        "    doc = filter(lambda x: x not in my_punctuation, doc)  \n",
        "    doc = filter(lambda x:not x.isdigit(), doc)\n",
        "    doc = [wnl.lemmatize(w.lower()) for w in doc]\n",
        "    #doc = [stem.stem(w) for w in doc]\n",
        "    doc = filter(lambda x:x not in stopwords, doc)\n",
        "    doc = filter(lambda x: x in word2vec_model.vocab or x in \".\",doc)\n",
        "    doc = ' '.join(e for e in doc if len(e)>=min_word_length)\n",
        "    # doc = ' '.join(e for e in doc)\n",
        "    return doc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SMctDoHEUC4"
      },
      "source": [
        "from gensim import models\n",
        "if local==True: \n",
        "  if not os.path.isfile(\"/home/grad16/sakumar/miniconda3/bin/GoogleNews-vectors-negative300.bin\"):\n",
        "    !wget -c \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\"\n",
        "    !gunzip GoogleNews-vectors-negative300.bin.gz\n",
        "  word2vec_model = models.KeyedVectors.load_word2vec_format('/home/grad16/sakumar/miniconda3/bin/GoogleNews-vectors-negative300.bin', binary=True)\n",
        "\n",
        "if local==False:\n",
        "  # https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?resourcekey=0-wjGZdNAUop6WykTtMip30g\n",
        "  !wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=0B7XkCwpI5KDYNlNUTTlSS21pQmM' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=0B7XkCwpI5KDYNlNUTTlSS21pQmM\" -O GoogleNews-vectors-negative300.bin.gz && rm -rf /tmp/cookies.txt\n",
        "  !gunzip GoogleNews-vectors-negative300.bin.gz\n",
        "  word2vec_model = models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9HqRCrQnbJl_"
      },
      "source": [
        "# Data --> Corpus & Labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ox0P8_ksb_P-"
      },
      "source": [
        "# %cd /content\n",
        "# !rm -r *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9dcSXXYBK-2s"
      },
      "source": [
        "# get_Data Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AviW5i5LLH_O"
      },
      "source": [
        "## get_corpus_labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nbh6z5YtTRt2"
      },
      "source": [
        "def get_yahoo_answers(dtype):\n",
        "  gdrive_fileid = \"\"\"0Bz8a_Dbh9Qhbd2JNdDBsQUdocVU\"\"\"\n",
        "  gdrive_filename = \"\"\"yahoo_answers_csv.tar.gz\"\"\"\n",
        "\n",
        "  if local:\n",
        "    home_dir = '/home/grad16/sakumar/colab/preprocessing'\n",
        "  if not local:\n",
        "    home_dir = '/content'\n",
        "\n",
        "  dir= home_dir+'/yahoo_answers_csv'\n",
        "  if not os.path.exists(dir) and not os.path.isfile(gdrive_filename):\n",
        "    !wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=0Bz8a_Dbh9Qhbd2JNdDBsQUdocVU' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=0Bz8a_Dbh9Qhbd2JNdDBsQUdocVU\" -O yahoo_answers_csv.tar.gz && rm -rf /tmp/cookies.txt\n",
        "    os.system(\"tar -xvzf yahoo_answers_csv.tar.gz\")\n",
        "  # os.makedirs(home_dir+dir,exist_ok=True)\n",
        "  os.chdir(dir)\n",
        "  dict_labels = {'1':'Society & Culture', '2': 'Science & Mathematics', \n",
        "                '3': 'Health', '4': 'Education & Reference', '5' : 'Computers & Internet',\n",
        "                '6': 'Sports', '7': 'Business & Finance', '8': 'Entertainment & Music',\n",
        "                '9': 'Family & Relationships' , '10':'Politics & Government'}\n",
        "\n",
        "  labels = []\n",
        "  corpus = []\n",
        "  files = ['train.csv','test.csv']\n",
        "  # files = ['train.csv']\n",
        "  count = 0\n",
        "  for f in files:\n",
        "    with open(f, newline='') as csvfile:\n",
        "        reader = csv.reader(csvfile)\n",
        "  #       # next(reader)\n",
        "        if dtype == 'short':\n",
        "          for row in reader:\n",
        "            count = count+1\n",
        "            labels.append(dict_labels[row[0]])\n",
        "            # corpus.append(' '.join(t for t in row[1:]))\n",
        "            corpus.append(' '.join(t for t in row[1:3])) # SHORT\n",
        "        elif dtype == \"full\": \n",
        "          for row in reader:\n",
        "            count = count+1\n",
        "            labels.append(dict_labels[row[0]])\n",
        "            corpus.append(' '.join(t for t in row[1:]))\n",
        "      \n",
        "  os.chdir(home_dir)\n",
        "  return corpus,labels\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sAUrDZ9ETPzC"
      },
      "source": [
        "# def sog_news()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "20yVS1UtTK8P"
      },
      "source": [
        "# def get_dbpedia()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_snT125GKlfN"
      },
      "source": [
        "def get_twentynews():\n",
        "  from sklearn.datasets import fetch_20newsgroups\n",
        "  data_20news=fetch_20newsgroups(subset='all',remove=('headers', 'footers', 'quotes'))\n",
        "  targets = data_20news.target\n",
        "  target_labels = data_20news.target_names \n",
        "  labels = [target_labels[t] for t in targets]\n",
        "  return data_20news.data , labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w-6noHLTLcJs"
      },
      "source": [
        "def get_NewsCategory():\n",
        "  os.system(\"wget -N https://www.dropbox.com/s/irvy3tdd305nhlr/news_category.zip\")\n",
        "  os.system(\"unzip News_Category.zip\")\n",
        "  df = pd.read_json('News_Category_Dataset_v2.json', lines=True)\n",
        "  labels = df['category'].values\n",
        "  corpus = df['headline'].values+\" . \"+df['short_description'].values\n",
        "  return corpus,labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5jUoHmghbLWQ"
      },
      "source": [
        "def get_AGNews_data_120k():\n",
        "  # try : os.mkdir(data_to_get)\n",
        "  # except FileExistsError:pass\n",
        "  # os.chdir(data_to_get)\n",
        "  if not os.path.isfile('ag_news_csv.tar.gz'):\n",
        "    os.system('wget -N -c https://www.dropbox.com/s/tyzue51quuo5y79/ag_news_csv.tar.gz')\n",
        "    os.system('tar -xvzf ag_news_csv.tar.gz')\n",
        "  if local:\n",
        "    home_dir = '/home/grad16/sakumar/colab/preprocessing'\n",
        "    dir= home_dir+'/ag_news_csv'\n",
        "    os.chdir(dir)\n",
        "    # os.chdir(colab_dir+'\\\\'+directory+'\\\\'+'AGNews')\n",
        "  else:\n",
        "    os.chdir('/content/ag_news_csv')\n",
        "  dict_labels = {'1':'World', '2': 'Sports', '3': 'Business', '4': 'Sci/Tech'}\n",
        "  labels = []\n",
        "  corpus = []\n",
        "  # files = ['train.csv','test.csv']\n",
        "  files = ['train.csv']\n",
        "  count = 0\n",
        "  for f in files:\n",
        "    with open(f, newline='') as csvfile:\n",
        "        reader = csv.reader(csvfile)\n",
        "        # next(reader)\n",
        "        for row in reader:\n",
        "          count = count+1\n",
        "          labels.append(row[0])\n",
        "          corpus.append(row[-1])\n",
        "        # corpus = corpus[1:]\n",
        "        # labels = labels[1:]\n",
        "  labels = [dict_labels[l] for l in labels]\n",
        "  os.chdir('..')\n",
        "  return corpus,labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJlPqqhnmFG0"
      },
      "source": [
        "def get_AGNews_data():\n",
        "  # try : os.mkdir(data_to_get)\n",
        "  # except FileExistsError:pass\n",
        "  # os.chdir(data_to_get)\n",
        "  # # os.system('wget -N -c https://md-datasets-public-files-prod.s3.eu-west-1.amazonaws.com/b0f38b56-b494-4074-9dfd-d80825cac772  -O WebOfScience.zip')\n",
        "  # os.system('unzip WebOfScience.zip')\n",
        "  if local:\n",
        "    os.chdir(colab_dir+'\\\\'+directory+'\\\\'+'AGNews')\n",
        "  dict_labels = {'1':'World', '2': 'Sports', '3': 'Business', '4': 'Sci/Tech'}\n",
        "  labels = []\n",
        "  corpus = []\n",
        "  files = ['train.csv','test.csv']\n",
        "  count = 0\n",
        "  for f in files:\n",
        "    with open(f, newline='') as csvfile:\n",
        "        reader = csv.reader(csvfile)\n",
        "        next(reader)\n",
        "        for row in reader:\n",
        "          count = count+1\n",
        "          labels.append(row[0])\n",
        "          corpus.append(row[-1])\n",
        "        # corpus = corpus[1:]\n",
        "        # labels = labels[1:]\n",
        "  labels = [dict_labels[l] for l in labels]\n",
        "  os.chdir('..')\n",
        "  return corpus,labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86huEaNGaKbP"
      },
      "source": [
        "def get_WOS46985_data(dtype):\n",
        "    # try : os.mkdir(data_to_get)\n",
        "    # except FileExistsError:pass\n",
        "    # os.chdir(data_to_get)\n",
        "  os.system('wget -N -c https://md-datasets-public-files-prod.s3.eu-west-1.amazonaws.com/b0f38b56-b494-4074-9dfd-d80825cac772  -O WebOfScience.zip')\n",
        "  os.system('unzip WebOfScience.zip')\n",
        "  \n",
        "  if local: os.chdir(colab_dir+'\\\\'+directory+'\\\\'+'WebOfScience\\\\WOS46985')\n",
        "  if local ==False: os.chdir('/content/WOS46985')\n",
        "  \n",
        "  dict_labels = {'0':'Computer  Science','1':'Electrical  Engineering', '2': 'Psychology', '3': 'Mechanical  Engineering',\n",
        "  '4': 'Civil  Engineering' , '5': 'Medical  Science', '6': 'biochemistry'}\n",
        "  if dtype=='full':  \n",
        "    with open('X.txt','r') as f:\n",
        "      content = f.readlines()\n",
        "      content = [x.strip().lower() for x in content]\n",
        "      corpus = content \n",
        "  elif dtype=='short':\n",
        "    os.chdir('/content/Meta-data')\n",
        "    WOS_keywords = np.array(pd.read_excel('Data.xlsx')['keywords'])\n",
        "    WOS_keywords = [w.strip() for w in WOS_keywords]\n",
        "    WOS_keywords = [w.split(';') for w in WOS_keywords]\n",
        "    WOS_keywords = [''.join(e) for e in WOS_keywords]\n",
        "    WOS_keywords = [w.strip() for w in WOS_keywords]\n",
        "    corpus = WOS_keywords\n",
        "  if local ==False: os.chdir('/content/WOS46985')\n",
        "  with open('YL1.txt','r') as f:\n",
        "      content = f.readlines()\n",
        "      content = [x.strip().lower() for x in content]\n",
        "      content = [dict_labels[x] for x in content]\n",
        "      labels = content\n",
        "  os.chdir('..')\n",
        "  return corpus,labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PaX5rp-8VRET"
      },
      "source": [
        "def get_5ktweets_data():\n",
        "  os.system('wget -N https://raw.githubusercontent.com/zfz/twitter_corpus/master/full-corpus.csv')\n",
        "  labels = []\n",
        "  corpus = []\n",
        "  with open('full-corpus.csv', newline='') as csvfile:\n",
        "      reader = csv.reader(csvfile)\n",
        "      for row in reader:\n",
        "        labels.append(row[0])\n",
        "        corpus.append(row[-1])\n",
        "      corpus = corpus[1:]\n",
        "      labels = labels[1:]\n",
        "      return corpus,labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QYKQSo-uEgr7"
      },
      "source": [
        "def get_bbc_data(dtype='full'):\n",
        "  os.system('wget -N https://www.dropbox.com/s/vunli21d312x55g/bbc.zip')\n",
        "  os.system('wget -N https://www.dropbox.com/s/h6y9zfdb76gl4uz/bbc-fulltext.zip')\n",
        "  os.system('unzip bbc.zip')\n",
        "  os.system('unzip bbc-fulltext.zip')\n",
        "\n",
        "  # BBC Docs -\n",
        "  corpus = []\n",
        "  subfolders = [f.path for f in os.scandir(os.getcwd()+'/bbc') if f.is_dir()]\n",
        "  subfolders = sorted(subfolders)\n",
        "  for s in subfolders:\n",
        "    files_list = sorted(glob.glob(s+\"/*.txt\")) \n",
        "    for file in files_list:\n",
        "      with open(file, \"rb\") as f:\n",
        "        content = f.readlines()\n",
        "        if dtype == 'short':\n",
        "          content = [content[0],content[2]] # headlines and abstracts \n",
        "        content = [x.strip().lower().decode('ISO-8859-1') for x in content] \n",
        "        corpus.append(' '.join(content).strip())\n",
        "\n",
        "  # BBC_labels -\n",
        "  with open(\"bbc.classes\", \"r\") as f:\n",
        "    content = f.readlines()\n",
        "    content = [x.strip()[::-1][0] for x in content] \n",
        "    labels = content[4:]\n",
        "    label_dict = {'0':'business','1':'entertainment','2':'politics','3':'sport','4':'tech'}\n",
        "  for l in range(len(labels)):\n",
        "    labels[l] = label_dict[labels[l]]\n",
        "  return corpus,labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_bmztqtwICZ_"
      },
      "source": [
        "def get_searchsnippet_data():\n",
        "  os.system('wget http://jwebpro.sourceforge.net/data-web-snippets.tar.gz')\n",
        "  os.system('tar -xvzf data-web-snippets.tar.gz')\n",
        "  corpus = []\n",
        "  labels=[]\n",
        "  with open(\"/content/data-web-snippets/train.txt\", \"r\") as f:\n",
        "    content = f.readlines()\n",
        "    content = [x.split('\\n')[0] for x in content]\n",
        "    labels_train = [x.split(' ')[-1] for x in content]\n",
        "    content = [' '.join(x.split(' ')[:-1]) for x in content]\n",
        "    corpus.extend(content)\n",
        "    labels.extend(labels_train)\n",
        "\n",
        "  with open(\"/content/data-web-snippets/test.txt\", \"r\") as f:\n",
        "    content = f.readlines()\n",
        "    content = [x.split('\\n')[0] for x in content]\n",
        "    labels_test = [x.split(' ')[-1] for x in content]\n",
        "    content = [' '.join(x.split(' ')[:-1]) for x in content]\n",
        "    corpus.extend(content)\n",
        "    labels.extend(labels_test)\n",
        "  return corpus,labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kj2CHjg6ZLUo"
      },
      "source": [
        "def get_stackoverflow_data():  \n",
        "  data_labels_dict = {1: 'wordpress', 2: 'oracle', 3: 'svn',4: 'apache', 5: 'excel',6: 'matlab',7: 'visual-studio',8: 'cocoa',9: 'osx',10: 'bash',11: 'spring',12: 'hibernate',13: 'scala',14: 'sharepoint',15: 'ajax',16: 'qt',17: 'drupal',18: 'linq',19: 'haskell',20: 'magento'}\n",
        "  os.system('wget -N https://raw.githubusercontent.com/jacoxu/StackOverflow/master/rawText/label_StackOverflow.txt')\n",
        "  os.system('wget -N https://raw.githubusercontent.com/jacoxu/StackOverflow/master/rawText/title_StackOverflow.txt')\n",
        "  corpus = []\n",
        "  labels=[]\n",
        "  with open(\"/content/title_StackOverflow.txt\", \"r\") as f:\n",
        "      content = f.readlines()\n",
        "      content = [x.split('\\n')[0] for x in content]\n",
        "      corpus.extend(content)\n",
        "  with open(\"/content/label_StackOverflow.txt\", \"r\") as f:\n",
        "      content = f.readlines()\n",
        "      content = [x.split('\\n')[0] for x in content]\n",
        "      # content = [data_labels_dict[int(x)] for x in content]\n",
        "      labels.extend(content)\n",
        "  return corpus,labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3ightnwLEN9"
      },
      "source": [
        "## get_Data_Driver"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W4YHH_BUTylm"
      },
      "source": [
        "def get_data(data_to_get,dtype):\n",
        "  data_to_get = data_to_get.lower()\n",
        "  if data_to_get == 'bbc':\n",
        "    corpus,labels = get_bbc_data(dtype)\n",
        "  elif data_to_get == 'searchsnippet':\n",
        "    corpus,labels = get_searchsnippet_data()\n",
        "  elif data_to_get == 'stackoverflow':\n",
        "    corpus,labels = get_stackoverflow_data()\n",
        "  elif data_to_get == '5ktweets':\n",
        "    corpus,labels = get_5ktweets_data()\n",
        "  elif data_to_get == 'wos':\n",
        "    corpus,labels = get_WOS46985_data(dtype)\n",
        "  elif data_to_get == 'agnews':\n",
        "    corpus,labels = get_AGNews_data()\n",
        "  elif data_to_get == 'newscategory':\n",
        "    corpus,labels = get_NewsCategory()\n",
        "  elif data_to_get == 'agnews120k':\n",
        "    corpus,labels = get_AGNews_data_120k()  \n",
        "  elif data_to_get == 'twentynews':\n",
        "    corpus,labels = get_twentynews()  \n",
        "  elif data_to_get == 'yahooanswers':\n",
        "    corpus,labels = get_yahoo_answers(dtype)\n",
        "  return corpus,labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9T9deqcLNRH"
      },
      "source": [
        "## doc label preprocessing | random sampling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8PJcZKG8QM1J"
      },
      "source": [
        "# def lookUpWord(vec,dtm,word):\n",
        "#     i = vec.get_feature_names().index(word)\n",
        "#     return dtm[:,i].nonzero()[0]\n",
        "\n",
        "# from sklearn.feature_extraction.text import CountVectorizer\n",
        "# import numpy as np\n",
        "\n",
        "# corpus = [\n",
        "# 'This is the first document.',\n",
        "# 'This is the second second document.',\n",
        "# 'And third one.',\n",
        "# 'Is this the first document?'\n",
        "# ]\n",
        "\n",
        "# X = CountVectorizer(max_features=2)\n",
        "# Y = X.fit_transform(corpus)\n",
        "# # lookUpWord(X,Y,'first')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UqgBMG4jE9rD"
      },
      "source": [
        "def vocab_filtered_data(doc,vocab):\n",
        "  doc = word_tokenize(doc)\n",
        "  doc = filter(lambda x: x in vocab, doc) \n",
        "  doc = ' '.join(e for e in doc)\n",
        "  return doc\n",
        "\n",
        "def docs_labels_preprocessing(docs,labels,word2vec_model,min_doc_len,min_word_length,max_features):\n",
        "  data_preprocessed = []\n",
        "  data_preprocessed_labels = []\n",
        "  embeddings = {}\n",
        "\n",
        "  for i in range(len(docs)):\n",
        "    doc = preprocessing(docs[i],word2vec_model,string.punctuation,min_word_length)\n",
        "    data_preprocessed.append(doc)\n",
        "    data_preprocessed_labels.append(labels[i])\n",
        " \n",
        "  vectorizer = CountVectorizer(max_features=max_features,dtype=np.float32)\n",
        "  train_vec = vectorizer.fit_transform(data_preprocessed).toarray()\n",
        "  vocab = vectorizer.vocabulary_\n",
        "\n",
        "  nonzeros_indexes = np.where(train_vec.any(1))[0]\n",
        "  data_preprocessed = [data_preprocessed[i] for i in nonzeros_indexes]\n",
        "  data_preprocessed_labels = [data_preprocessed_labels[i] for i in nonzeros_indexes]\n",
        "\n",
        "  for i in range(len(data_preprocessed)):\n",
        "    data_preprocessed[i] = vocab_filtered_data(data_preprocessed[i],vocab) \n",
        "\n",
        "  data_preprocessed_f = [data_preprocessed[i] for i in range(len(data_preprocessed)) if len(data_preprocessed[i].split(' '))>=min_doc_len]\n",
        "  data_preprocessed_labels_f = [data_preprocessed_labels[i] for i in range(len(data_preprocessed)) if len(data_preprocessed[i].split(' '))>=min_doc_len]\n",
        "  \n",
        "  vectorizer_f = CountVectorizer(dtype=np.float32)\n",
        "  vectorizer_f.fit_transform(data_preprocessed_f)\n",
        "  vocab_f = vectorizer.vocabulary_\n",
        "\n",
        "  for f in vocab:\n",
        "    embeddings[f] = word2vec_model[f]\n",
        "  return data_preprocessed_f,data_preprocessed_labels_f,embeddings,vocab_f"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mxKMOwoDezy0"
      },
      "source": [
        "def docs_random_sample(data_preprocessed,data_preprocessed_labels,sample_size):\n",
        "  ## BBC - 250 random docs sampling (50 from each label)\n",
        "  \n",
        "  df = pd.DataFrame(data_preprocessed)\n",
        "  df['classes'] = data_preprocessed_labels\n",
        "  df = df.groupby('classes').apply(lambda x: x.sample(sample_size))\n",
        "  index = list(df.index)\n",
        "  ret_index  = np.asarray([i[1] for i in index])\n",
        "\n",
        "  data_prepocessed_sampled= df.values[:,0]\n",
        "  data_prepocessed_labels_sampled = df.values[:,1]\n",
        "  # index = np.random.choice(df.values[:,0].shape[0], sample_size*len(set(data_preprocessed_labels)), replace=False)  \n",
        "  # data_prepocessed_sampled = data_prepocessed_sampled[index]\n",
        "  # data_prepocessed_labels_sampled = data_prepocessed_labels_sampled[index]\n",
        "  return data_prepocessed_sampled,data_prepocessed_labels_sampled,ret_index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lx5AaCa96wyu"
      },
      "source": [
        "# **DATA GENERATION**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IKBxLlmxFaWE"
      },
      "source": [
        "# %cd /content/\n",
        "# !rm -r *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oy6hiXZXKGRz"
      },
      "source": [
        "## Driver - Data Generation\n",
        "# all_data_to_get = ['bbc','searchsnippet','wos'] # bbc,searchsnippet,stackoverflow,5ktweets,wos(Web Of Science)\n",
        "                                                  # agnews,agnews120k,twentynews,yahooanswers\n",
        "all_data_to_get = ['yahooanswers']\n",
        "dtypes = ['short']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "ZISpZSCRWC8R"
      },
      "source": [
        "#@title Short Data gen\n",
        "def short_data_gen(data_preprocessed,data_preprocessed_labels,embeddings,f):\n",
        "  os.system('mkdir -p /content/data_'+data_to_get+\"/short\")\n",
        "  os.chdir('/content/data_'+data_to_get+\"/short\")\n",
        "\n",
        "  data_preprocessed_short = []\n",
        "  data_preprocessed_labels_short = []\n",
        "\n",
        "  for doc,label in zip(data_preprocessed,data_preprocessed_labels):\n",
        "    if len(doc.split(' ')) <= short_len_doc:\n",
        "      data_preprocessed_short.append(doc)\n",
        "      data_preprocessed_labels_short.append(label)\n",
        "\n",
        "  vectorizer = CountVectorizer(dtype=np.float32)\n",
        "  train_vec = vectorizer.fit_transform(data_preprocessed_short).toarray()\n",
        "  vocab_short = vectorizer.vocabulary_\n",
        "\n",
        "  f.write('\\n\\nlen of - \\n  data_preprocessed_short: '+str(len(data_preprocessed_short))+'\\n  data_preprocessed_labels_short: '+str(len(data_preprocessed_labels_short))+'\\n  vocab_short:  '+str(len(vocab_short))+'\\n')\n",
        "  len_docs_short = [len(d.split(\" \")) for d in data_preprocessed_short]\n",
        "  f.write('\\n min,mean,max short docs len: '+str(np.min(len_docs_short))+', '+str(np.mean(len_docs_short).round(2))+', '+str(np.max(len_docs_short))+'\\n\\n')\n",
        "  f.write('**'*50)\n",
        "  f.write(\"\\n\")\n",
        "\n",
        "  save_obj(data_preprocessed_short,'data_preprocessed'+'_'+data_to_get+'_short')\n",
        "  save_obj(data_preprocessed_labels_short,'data_preprocessed_labels'+'_'+data_to_get+'_short')\n",
        "  save_obj(vocab_short,'vocab'+'_'+data_to_get+'_short') \n",
        "  save_obj(embeddings,'embeddings'+'_'+data_to_get+'_short')  \n",
        "\n",
        "  os.chdir('/content/data_'+data_to_get+\"/\"+dtype)\n",
        "  return data_preprocessed_short,data_preprocessed_labels_short,vocab_short,f"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hDWtBebM6vri",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "outputId": "6c91ec47-2250-4424-d2d7-73c822b1475e"
      },
      "source": [
        "## Settings :- \n",
        "min_doc_len = 3\n",
        "min_word_length = 3\n",
        "short_len_doc = 21\n",
        "\n",
        "if local:\n",
        "  home_dir = '/home/grad16/sakumar/colab/preprocessing'\n",
        "elif not local:\n",
        "  home_dir = '/'\n",
        "# min_df=3\n",
        "# if data_to_get == 'bbc': max_features = 2000\n",
        "# elif data_to_get == 'searchsnippet': max_features = 3000\n",
        "# elif data_to_get == 'wos': max_features = 4000\n",
        "\n",
        "for data_to_get in all_data_to_get:\n",
        "\n",
        "  if data_to_get == 'bbc': max_features = 2000\n",
        "  elif data_to_get == 'searchsnippet': max_features = 3000\n",
        "  elif data_to_get == 'wos': max_features = 4000\n",
        "  elif data_to_get == 'newscategory': max_features = 4000\n",
        "  elif data_to_get == 'agnews120k': max_features = 8000\n",
        "  elif data_to_get == 'yahooanswers': max_features = 8000\n",
        "  else: max_features = 4000\n",
        "  \n",
        "  for dtype in dtypes:\n",
        "    if data_to_get=='searchsnippet' and dtype=='full':\n",
        "      continue\n",
        "\n",
        "    # os.chdir(\"/content\")\n",
        "    docs,labels = get_data(data_to_get,dtype)\n",
        "    os.makedirs(home_dir+'/content/data_'+data_to_get+\"/\"+dtype,exist_ok=True)\n",
        "    data_to_get\n",
        "\n",
        "    with open(data_to_get+'_'+dtype+\".txt\", \"a\") as f:\n",
        "      f.write(data_to_get+\" - \"+dtype)\n",
        "      f.write(\"\\n\\n\")\n",
        "      f.write('**'*50)\n",
        "      f.write(\"\\n\\n\")\n",
        "      f.write(str(len(docs))+', '+str(len(labels))+'\\n')\n",
        "      f.write('(labels,count): '+str(list(zip(*np.unique(labels, return_counts=True)))))\n",
        "      data_preprocessed,data_preprocessed_labels,embeddings,vocab = docs_labels_preprocessing(docs,labels,word2vec_model,min_doc_len,min_word_length,max_features)\n",
        "      f.write('\\n\\nlen of - \\n  data_preprocessed: '+str(len(data_preprocessed))+'\\n  data_preprocessed_labels: '+str(len(data_preprocessed_labels))+'\\n  vocab:  '+str(len(vocab))+'\\n  embeddings : '+str(len(embeddings))+'\\n')\n",
        "      len_docs = [len(d.split(\" \")) for d in data_preprocessed]\n",
        "      f.write('\\n min,mean,max docs len: '+str(np.min(len_docs))+', '+str(np.mean(len_docs).round(2))+', '+str(np.max(len_docs))+'\\n\\n')\n",
        "      \n",
        "      save_obj(data_preprocessed,'data_preprocessed'+'_'+data_to_get+'_'+dtype)\n",
        "      save_obj(data_preprocessed_labels,'data_preprocessed_labels'+'_'+data_to_get+'_'+dtype)   \n",
        "      save_obj(vocab,'vocab'+'_'+data_to_get+'_'+dtype)\n",
        "      save_obj(embeddings,'embeddings'+'_'+data_to_get+'_'+dtype)\n",
        "\n",
        "      f.write('**'*50)\n",
        "\n",
        "      ### short ####\n",
        "\n",
        "      # data_preprocessed_short,data_preprocessed_labels_short,vocab_short,f = short_data_gen(data_preprocessed,data_preprocessed_labels,embeddings,f)\n",
        "\n",
        "      ######\n",
        "\n",
        "      # sample_size =20 # 20,50\n",
        "      # name = 'small'\n",
        "      # f.write(data_to_get+'_'+dtype+'_'+name+'\\n\\n\\n\\n:')\n",
        "      # for i in range(1):\n",
        "      #   id = str(i+1)\n",
        "      #   d_data= data_to_get\n",
        "      #   f.write('\\n\\nid : '+id+'\\n\\n')\n",
        "\n",
        "      #   ## Small\n",
        "      #   data_prepocessed_sampled,data_prepocessed_labels_sampled,index = docs_random_sample(data_preprocessed,data_preprocessed_labels,sample_size)\n",
        "      #   f.write(\"len of data_preprocesssed_sampled: \"+str(len(data_prepocessed_sampled)))\n",
        "      #   f.write('\\n(labels,count): '+str(list(zip(*np.unique(data_prepocessed_labels_sampled, return_counts=True)))))\n",
        "      #   len_docs = [len(d.split(\" \")) for d in data_prepocessed_sampled]\n",
        "      #   vectorizer = CountVectorizer(dtype=np.float32)\n",
        "      #   train_vec_sampled = vectorizer.fit_transform(data_prepocessed_sampled).toarray()\n",
        "      #   vocab_sampled = vectorizer.vocabulary_\n",
        "      #   f.write('\\n min,mean,max docs len: '+str(np.min(len_docs))+', '+str(np.mean(len_docs).round(2))+', '+str(np.max(len_docs)))\n",
        "      #   f.write('\\n sampled_data_vocab_size: '+str(len(vocab_sampled)))\n",
        "\n",
        "      #   save_obj(data_prepocessed_sampled,id+'_docs_sampled_'+name+'_'+str(sample_size)+d_data+dtype)\n",
        "      #   save_obj(data_prepocessed_labels_sampled,id+'_labels_sampled_'+name+'_'+str(sample_size)+d_data+dtype)\n",
        "      #   save_obj(index,id+'_index_'+name+'_'+str(sample_size)+d_data+dtype)\n",
        "      # os.chdir(home_dir)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-77-48d6308a3524>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;31m# os.chdir(\"/content\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mdocs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_to_get\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhome_dir\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/content/data_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mdata_to_get\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"/\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mdata_to_get\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-23-5650f25c8de1>\u001b[0m in \u001b[0;36mget_data\u001b[0;34m(data_to_get, dtype)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_twentynews\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0mdata_to_get\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'yahooanswers'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_yahoo_answers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-d01a39efc7e3>\u001b[0m in \u001b[0;36mget_yahoo_answers\u001b[0;34m(dtype)\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0;31m# corpus.append(' '.join(t for t in row[1:]))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# SHORT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"full\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m           \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-d01a39efc7e3>\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0;31m# corpus.append(' '.join(t for t in row[1:]))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# SHORT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"full\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m           \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMtZjheUKUyg"
      },
      "source": [
        "# os.chdir(\"/content\")\n",
        "# from google.colab import files\n",
        "os.chdir(home_dir)\n",
        "data_to_get = 'yahooanswers'\n",
        "zipped_pickle_filename = data_to_get\n",
        "if local==True:\n",
        "  os.system('zip -r '+zipped_pickle_filename+'_'+str(max_features)+'_.zip '+'./content/data_'+data_to_get+\"/\"+dtype)\n",
        "elif local==False:\n",
        "  os.system('zip -r '+zipped_pickle_filename+'_'+str(max_features)+'_.zip '+'/content/data_'+data_to_get+\"/\"+dtype)\n",
        "\n",
        "# files.download(zipped_pickle_filename+'_'+str(max_features)+'_.zip')\n",
        "\n",
        "# os.system('zip -r '+zipped_pickle_filename+'_'+str(max_features)+'_'+str(sample_size)+'.zip /content/data_'+data_to_get)\n",
        "# files.download(zipped_pickle_filename+'_'+str(max_features)+'_'+str(sample_size)+'.zip')\n",
        "\n",
        "# os.system(\"rm -r *\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SmeccYkpp_4B"
      },
      "source": [
        "# Sampling of data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yrUx3zKGlZ1C",
        "outputId": "edc1b2a9-474f-4016-b313-a7f8a2d2bdf4"
      },
      "source": [
        "%cd /content\n",
        "!rm -r *"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o4JxPcUP_KOK",
        "outputId": "ea6333b6-d7e1-4e85-aa55-d0bdebb02833"
      },
      "source": [
        "!unzip yahooanswers_8000_.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  yahooanswers_8000_.zip\r\n",
            "   creating: content/data_yahooanswers/short/\r\n",
            "  inflating: content/data_yahooanswers/short/yahooanswers_short.txt  \r\n",
            "   creating: content/data_yahooanswers/short/.ipynb_checkpoints/\r\n",
            "  inflating: content/data_yahooanswers/short/data_preprocessed_labels_yahooanswers_short.pkl  \n",
            "  inflating: content/data_yahooanswers/short/data_preprocessed_yahooanswers_short.pkl  \n",
            "  inflating: content/data_yahooanswers/short/embeddings_yahooanswers_short.pkl  \n",
            "  inflating: content/data_yahooanswers/short/vocab_yahooanswers_short.pkl  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ZKXTDikgWnb"
      },
      "source": [
        "# !pip install pickle5\n",
        "# import pickle5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q9B3jtOGfzjT"
      },
      "source": [
        "# !wget https://www.dropbox.com/s/77a814l596kdz8b/yahooanswers_4000_short_complete.zip\n",
        "# !unzip yahooanswers_4000_short_complete.zip\n",
        "os.chdir(home_dir+'/content/data_'+data_to_get+\"/\"+dtype)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8kKooyfK_otk",
        "outputId": "86ff1c1e-ff70-4a17-9a78-2e4e309f5cca"
      },
      "source": [
        "pwd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/home/grad16/sakumar/colab/preprocessing/content/data_yahooanswers/short'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3uCvZLttgEtu"
      },
      "source": [
        "data_preprocessed = load_obj_pkl5('data_preprocessed_yahooanswers_short')\n",
        "data_preprocessed_labels = load_obj_pkl5('data_preprocessed_labels_yahooanswers_short')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UCtSvKk1AP0W",
        "outputId": "8ba1ce61-1736-4b72-ab3e-149aabe8c62b"
      },
      "source": [
        "pwd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/home/grad16/sakumar/colab/preprocessing/content/data_yahooanswers/short'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hvrt6VymAe1Q"
      },
      "source": [
        "home_dir = '/home/grad16/sakumar/colab/preprocessing'\n",
        "os.chdir(home_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NlyADx6xfq0n"
      },
      "source": [
        "# %cd /content\n",
        "# !rm -r *\n",
        "\n",
        "sample_size = 5000 # 20,50\n",
        "# home_dir =''\n",
        "all_data_to_get = ['yahooanswers']\n",
        "dtypes = ['short']\n",
        "\n",
        "data_to_get = all_data_to_get[0]\n",
        "dtype = dtypes[0]\n",
        "\n",
        "os.chdir(home_dir)\n",
        "os.makedirs(home_dir+'/content/data_'+data_to_get+\"/\"+dtype+'/sample',exist_ok=True)\n",
        "os.chdir(home_dir+'/content/data_'+data_to_get+\"/\"+dtype+'/sample')\n",
        "\n",
        "with open(data_to_get+'_'+dtype+\".txt\", \"a\") as f:\n",
        "  f.write(data_to_get+'_'+dtype+'_\\n\\n\\n\\n:')\n",
        "  for i in range(1):\n",
        "    id = str(i+1)\n",
        "    d_data= data_to_get\n",
        "    ## Small\n",
        "    data_prepocessed_sampled,data_prepocessed_labels_sampled,index = docs_random_sample(data_preprocessed,data_preprocessed_labels,sample_size)\n",
        "    f.write(\"len of data_preprocesssed_sampled: \"+str(len(data_prepocessed_sampled)))\n",
        "    f.write('\\n(labels,count): '+str(list(zip(*np.unique(data_prepocessed_labels_sampled, return_counts=True)))))\n",
        "    len_docs = [len(d.split(\" \")) for d in data_prepocessed_sampled]\n",
        "    vectorizer = CountVectorizer(dtype=np.float32)\n",
        "    train_vec_sampled = vectorizer.fit_transform(data_prepocessed_sampled).toarray()\n",
        "    vocab_sampled = vectorizer.vocabulary_\n",
        "    embeddings_sampled ={}\n",
        "    for v in vocab_sampled:\n",
        "      embeddings_sampled[v] = word2vec_model[v]\n",
        "\n",
        "    f.write('\\n min,mean,max docs len: '+str(np.min(len_docs))+', '+str(np.mean(len_docs).round(2))+', '+str(np.max(len_docs)))\n",
        "    f.write('\\n sampled_data_vocab_size: '+str(len(vocab_sampled)))\n",
        "\n",
        "    save_obj(data_prepocessed_sampled,'data_preprocessed'+'_'+data_to_get+'_'+dtype+'_'+str(sample_size))\n",
        "    save_obj(data_prepocessed_labels_sampled,'data_preprocessed_labels'+'_'+data_to_get+'_'+dtype+'_'+str(sample_size))\n",
        "    save_obj(vocab_sampled,'vocab'+'_'+data_to_get+'_'+dtype+'_'+str(sample_size))\n",
        "    save_obj(embeddings_sampled,'embeddings'+'_'+data_to_get+'_'+dtype+'_'+str(sample_size))\n",
        "    save_obj(index,'index_'+data_to_get+\"_\"+dtype+'_'+str(sample_size))\n",
        "  \n",
        "  # save_obj(data_prepocessed_sampled,id+'_docs_sampled_'+name+'_'+str(sample_size)+d_data+dtype)\n",
        "  # save_obj(data_prepocessed_labels_sampled,id+'_labels_sampled_'+name+'_'+str(sample_size)+d_data+dtype)\n",
        "  # save_obj(index,id+'_index_'+name+'_'+str(sample_size)+d_data+dtype)\n",
        "\n",
        "# os.chdir('/content')\n",
        "os.chdir(home_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lsjUy7hDqDRc",
        "outputId": "36cb4777-d2ff-4c5c-9043-d89113319cca"
      },
      "source": [
        "data_prepocessed_sampled"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['doe rogers school district school today',\n",
              "       'renter utility doe generally utility included rent',\n",
              "       'sex man grew breast', ...,\n",
              "       'consider best player time broken surface lot work answer player win big match player multiple time man grass man clay man hard court double dream team match win marble woman',\n",
              "       'join wwe doe sounded good write sounded good fight punk jeff hardy',\n",
              "       'winner age religion'], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_V9I8WCzqEXn",
        "outputId": "48db6d24-38bf-4c62-c515-2b0b264a96d8"
      },
      "source": [
        "np.asarray(data_preprocessed)[index]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['doe rogers school district school today',\n",
              "       'renter utility doe generally utility included rent',\n",
              "       'sex man grew breast', ...,\n",
              "       'consider best player time broken surface lot work answer player win big match player multiple time man grass man clay man hard court double dream team match win marble woman',\n",
              "       'join wwe doe sounded good write sounded good fight punk jeff hardy',\n",
              "       'winner age religion'], dtype='<U2128')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Ky7IE22qR8S"
      },
      "source": [
        "from google.colab import files\n",
        "zipped_pickle_filename = data_to_get+'_sampled_'+str(sample_size)\n",
        "if local: \n",
        "  os.system('zip -r '+zipped_pickle_filename+'.zip ./content/data_'+data_to_get+\"/\"+dtype+'/sample') \n",
        "if not local:\n",
        "  os.system('zip -r '+zipped_pickle_filename+'_sampled_'+str(sample_size)+'.zip '+home_dir+'/content/data_'+data_to_get+\"/\"+dtype)\n",
        "files.download(zipped_pickle_filename+'_'+str(sample_size)+'_sampled.zip')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}